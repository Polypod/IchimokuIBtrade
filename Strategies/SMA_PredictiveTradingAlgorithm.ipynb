{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "114233a2",
   "metadata": {},
   "source": [
    "Predictive Trading Algorithm using TensorFlow\n",
    "\n",
    "- Install/Import our packages\n",
    "- Set up model parameters\n",
    "- Retrieve some data using Finnhub API\n",
    "- Setup the MACross Backtrading strategy in Backtrader\n",
    "- Find optimal parameters and get a baseline performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca4a097",
   "metadata": {},
   "source": [
    "Install and Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a91ae1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henrik/.pyenv/versions/3.9.4/lib/python3.9/site-packages/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f4452a8bfcbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkers\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Install Needed Packages. If using Google Collab these need to be in seperate cells.\n",
    "#!pip install finnhub-python\n",
    "#!pip install keras-tuner\n",
    "#!pip install backtrader\n",
    "#!pip install pandas\n",
    "#!pip install matplotlib\n",
    "\n",
    "# Import needed libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.markers as mark\n",
    "import tensorflow as tf\n",
    "import sys \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "from pandas_datareader import data as web\n",
    "import finnhub\n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "from time import time, sleep\n",
    "import requests\n",
    "from google.colab import drive\n",
    "from google.colab import files\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow import keras\n",
    "from kerastuner import HyperModel\n",
    "from kerastuner.tuners import RandomSearch \n",
    "from kerastuner.tuners import Hyperband \n",
    "from pylab import rcParams\n",
    "\n",
    "rcParams['figure.figsize'] = 16,9\n",
    "rcParams['figure.facecolor'] = '#eeeeee'\n",
    "plt.title('dummy')\n",
    "plt.plot([1,3,2,4])\n",
    "plt.close()\n",
    "\n",
    "from __future__ import (absolute_import, division, print_function,\n",
    "                        unicode_literals)\n",
    "\n",
    "import backtrader as bt\n",
    "import argparse\n",
    "import backtrader.feeds as btfeeds\n",
    "import os.path  # To manage paths\n",
    "import sys  # To find out the script name (in argv[0])\n",
    "\n",
    "# Have all of our plots show automatically\n",
    "%matplotlib inline\n",
    "\n",
    "# Mount your Google Drive to the worksheet\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed7776f",
   "metadata": {},
   "source": [
    "Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428c93f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Parameters\n",
    "ticker = \"CVX\"\n",
    "granularity = \"D\"\n",
    "\n",
    "# The amount of time divisions the LSTM model will look back for predictions\n",
    "lookback_distance = 60\n",
    "\n",
    "# Time Period for Stratagy Tuning\n",
    "dt_start_model_parameter_tuning = datetime(2000, 10, 19)\n",
    "dt_end_model_parameter_tuning = datetime(2006, 12, 31)\n",
    "\n",
    "# Time Period for Model Building\n",
    "dt_start_nn_model_training = datetime(2007, 1,1)\n",
    "dt_end_nn_model_training = datetime(2013, 12, 1)\n",
    "\n",
    "# Time Period for Backtesting\n",
    "dt_start_backtest = datetime(2014, 1,1)\n",
    "dt_end_backtest = datetime(2020, 12, 9)\n",
    "\n",
    "# SMA Cross Stratagy Search Grid. This defines the windows we will test to find an optimal \n",
    "# fast and slow window combination.\n",
    "pslow_start  = 30;\n",
    "pslow_end    = 200;\n",
    "pslow_step   = 10;\n",
    "\n",
    "pfast_start  = 30;\n",
    "pfast_end    = 200;\n",
    "pfast_step   = 10;\n",
    "\n",
    "# API Keys. Go to the finnhub website to get your API key and put it here.\n",
    "finnhub_api_key = \"Your Key Here\";\n",
    "\n",
    "# Drive Path\n",
    "google_drive_path = \"/content/drive/My Drive/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d8e761",
   "metadata": {
    "tags": []
   },
   "source": [
    "Retrieve and Setup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7b3a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covert the dates to UTC timestamps that will be used by the Finnhub API call\n",
    "timestamp_start = int(dt_start_model_parameter_tuning.replace(tzinfo=timezone.utc).timestamp())\n",
    "timestamp_end = int(dt_end_model_parameter_tuning.replace(tzinfo=timezone.utc).timestamp())\n",
    "\n",
    "# Initialize the Client\n",
    "finnhub_client = finnhub.Client(api_key=finnhub_api_key)\n",
    "\n",
    "# Get data and put into dataframe.\n",
    "res = finnhub_client.stock_candles(ticker, granularity, timestamp_start, timestamp_end)\n",
    "data = pd.DataFrame(res)\n",
    "\n",
    "# Make Data Backtrader Friendly\n",
    "dataBacktrader = data\n",
    "dataBacktrader.columns = ['close', 'high', 'low','open','s','t','volume']\n",
    "\n",
    "dataBacktrader['date'] = dataBacktrader.apply(lambda row : datetime.utcfromtimestamp(row['t']), axis = 1)\n",
    "dataBacktrader.drop(['s','t'], axis = 1, inplace=True)\n",
    "dataBacktrader['openinterest'] = 0\n",
    "\n",
    "# Plot the data\n",
    "plt.plot(dataBacktrader['close'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf0940c",
   "metadata": {},
   "source": [
    "Setup SmaCross Model\n",
    "\n",
    "Strategy already constructed in the Backtrader quick start page. \n",
    "One line added which checks correct parameters otherwise it stops the run to not waste compute time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13b9e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subclass of Strategy to define the indicators and logic\n",
    "class SmaCross(bt.Strategy):\n",
    "    # list of parameters which are configurable for the strategy\n",
    "    params = dict(\n",
    "        pfast=60,  # period for the fast moving average window\n",
    "        pslow=128   # period for the slow moving average window\n",
    "    )\n",
    "    \n",
    "    # The __init__ method runs at the start of any stratagy.\n",
    "    def __init__(self):\n",
    "        sma1 = bt.ind.SMA(period=self.p.pfast)  # fast moving average\n",
    "        sma2 = bt.ind.SMA(period=self.p.pslow)  # slow moving average\n",
    "        \n",
    "        # Only want to test valid parameters (and workable parameters)\n",
    "        # otherwise skip the run\n",
    "        if ((self.p.pfast <= self.p.pslow) and (self.p.pslow - self.p.pfast >= 5)) :\n",
    "          self.crossover = bt.ind.CrossOver(sma1, sma2)  # crossover signal\n",
    "        else :\n",
    "          raise bt.StrategySkipError   \n",
    "\n",
    "    # The next() method is run each step of the stratagy. \n",
    "    # The step of a stratagy is determined by the dataframe and the duraction \n",
    "    # between each data point.\n",
    "    def next(self):\n",
    "        if not self.position:  # not in the market\n",
    "            if self.crossover > 0:  # if fast crosses slow to the upside\n",
    "                self.buy()  # enter long\n",
    "\n",
    "        elif self.crossover < 0:  # in the market & cross to the downside\n",
    "            self.close()  # close long position\n",
    "            \n",
    "    # The stop method runs at the end of a stratagy run. \n",
    "    def stop(self):\n",
    "        print('(FA Period %2d) (SL Period %2d) Ending Value %.2f' %\n",
    "                 (self.params.pfast, self.params.pslow, self.broker.getvalue()))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d03c5d",
   "metadata": {},
   "source": [
    "Run SMACross Strategy and Baseline Performance\n",
    "\n",
    "Build and run a Cereboro object so that Backtrader will run our strategy over a set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b136dbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create array to store the results from all the backtrader runs.\n",
    "results_list = []\n",
    "\n",
    "# First we want to initialize our Cerebro object.\n",
    "cerebro = bt.Cerebro(stdstats=False,maxcpus=None)\n",
    "\n",
    "# Next we set the amount of cash we want to start out with.\n",
    "cerebro.broker.setcash(100000.0)\n",
    "\n",
    "# Next we read in the dataframe with the data we want backtest\n",
    "dataFrame = bt.feeds.PandasData(dataname=dataBacktrader, datetime='date')\n",
    "cerebro.adddata(dataFrame)\n",
    "\n",
    "# Now we load the actual stratagy. Since we want to run the stratagy over a large\n",
    "# combination of parameters we want to use the optstratagy method. We\n",
    "# define the range of parameters we want to run over based on the additional arguments.\n",
    "# Here we are running our stratagy over:\n",
    "# - pFast = pfast_start to pfast_end, with a stepsize of pfast_step\n",
    "# - pSlow = pslow_start to pslow_end, with a stepsize of pslow_step\n",
    "cerebro.optstrategy(SmaCross, pslow=range(pslow_start,pslow_end+1,pslow_step), pfast=range(pfast_start,pfast_end+1,pfast_step))\n",
    "\n",
    "# Next we add a sizer to the cerebro object. This determines the strategy of how large our buy and \n",
    "# sell orders are. Here we fix it top 99% of our portfolio. \n",
    "cerebro.addsizer(bt.sizers.PercentSizer, percents=99)\n",
    "\n",
    "# We are almost there but we now need to add analyzers to the cerebro object.\n",
    "# These analyzers allow us to extract information about how well the stratagy performed. \n",
    "# (Links to more info about these analyzers at the bottom of the post)\n",
    "cerebro.addanalyzer(bt.analyzers.SharpeRatio_A, _name=\"sharperatio\", timeframe=bt.TimeFrame.Days)\n",
    "cerebro.addanalyzer(bt.analyzers.TimeReturn, timeframe=bt.TimeFrame.NoTimeFrame)\n",
    "cerebro.addanalyzer(bt.analyzers.DrawDown)\n",
    "cerebro.addanalyzer(bt.analyzers.SQN, _name=\"sqn\")\n",
    "\n",
    "# Finally we want to set how much each tranasction will cost in commision fees. Since I personally\n",
    "# use Robinhood Ill just set this to 0.\n",
    "cerebro.broker.setcommission(commission=0)\n",
    "\n",
    "# Finally we run the model. This will run the stratagy over the wide array of parameters\n",
    "# we set above. An object is returned which holds all of the results of the runs.\n",
    "# (And the information we told it to gather from our Analyzers)\n",
    "results = cerebro.run();\n",
    "\n",
    "# Next we want to run over this results object and extract the information so it is easily\n",
    "# readable and indexable. \n",
    "for i in range(0, len(results)):  \n",
    "  # Need to check that this is a run that fully ran\n",
    "  if (len(results[i]) >= 1) :\n",
    "    # Temporarily store all the analyzer values into variables. \n",
    "    r_strat  = \"slow_fast_\" + str(results[i][0].params.pslow) + \"_\" + str(results[i][0].params.pfast);\n",
    "    r_return = list(results[i][0].analyzers.timereturn.get_analysis().values())[0]*100\n",
    "    r_sharpe = results[i][0].analyzers.sharperatio.get_analysis()['sharperatio'];\n",
    "    r_draw   = results[i][0].analyzers.drawdown.get_analysis()['drawdown']/100;\n",
    "    r_sqn   = results[i][0].analyzers.sqn.get_analysis()['sqn']\n",
    "\n",
    "    # Append a dictionary which holds all the important results into our results array.\n",
    "    results_list.append({\n",
    "       \"Stratagy\" : r_strat,\n",
    "       \"pslow\" : results[i][0].params.pslow,\n",
    "       \"pfast\" : results[i][0].params.pfast,\n",
    "       \"Return\" : r_return,\n",
    "       \"Sharpe\" : r_sharpe,\n",
    "       \"SQN\" : r_sqn,\n",
    "       \"Drawdown\" : r_draw\n",
    "    })\n",
    "\n",
    "\n",
    "# Make the results a Pandas dataframe and give us statsitical information \n",
    "# about the values of the runs. \n",
    "results = pd.DataFrame(results_list)               \n",
    "results.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd1c043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#        pslow       pfast       Return      Sharpe      SQN          Drawdown\n",
    "#count   153.000000  153.000000  153.000000  153.000000  153.000000  153.000000\n",
    "#mean    146.666667  83.333333   55.581273   0.499984    0.710518    0.052910\n",
    "#std     41.231056   41.231056   20.770793   0.156723    0.353140    0.045569\n",
    "#min     40.000000   30.000000   3.309254    0.048054    -0.034536   0.031449\n",
    "#25%     120.000000  50.000000   40.986669   0.393614    0.472027    0.031853\n",
    "#50%     150.000000  80.000000   53.110563   0.485314    0.658597    0.031938\n",
    "#75%     180.000000  110.000000  69.834780   0.604150    0.960126    0.044747\n",
    "#max     200.000000  190.000000  126.805272  0.983955    1.684354    0.262238"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e30b71",
   "metadata": {},
   "source": [
    "Values:\n",
    "\n",
    "- pslow - duration of the slow-moving average window'\n",
    "- pfast - duration of the fast-moving average window\n",
    "- Return - Percentage return of our strategy.\n",
    "- Annualized Sharpe - A metric that takes into account the return above, the returns from a risk-free investment, and the standard deviation of our returns. The Sharpe ratio gives us a better idea of how good a strategy is as it takes into account the risk and volatility of a strategy. I would suggest reading up some more on the Sharpe ratio.\n",
    "- SQN — This number takes into account the average profit from a trade, the variability of the profit, and how many trades you did. (From Backtrader Docs ->) System Quality Number defined by Van K. Tharp to categorize trading systems.\n",
    "1.6 - 1.9 Below average\n",
    "2.0 - 2.4 Average\n",
    "2.5 - 2.9 Good\n",
    "3.0 - 5.0 Excellent\n",
    "5.1 - 6.9 Superb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b68e70",
   "metadata": {},
   "source": [
    "Create a heatmap of the Sharpe values to see any patterns\n",
    "Sharpe takes into account not just the return but also the variation in return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a72bf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data frame where the rows are pslow and the columns are pfast.\n",
    "heat_df = pd.DataFrame(columns=results['pfast'].drop_duplicates(keep='first', inplace=False))\n",
    "row = results['pslow'].drop_duplicates(keep='first', inplace=False)\n",
    "heat_df['pslow'] = row\n",
    "heat_df = heat_df.set_index('pslow')\n",
    "heat_df = pd.DataFrame(columns=results['pfast'].drop_duplicates(keep='first', inplace=False))\n",
    "row = results['pslow'].drop_duplicates(keep='first', inplace=False)\n",
    "heat_df['pslow'] = row\n",
    "heat_df = heat_df.set_index('pslow')\n",
    "\n",
    "heat_df.columns.name = None\n",
    "heat_df.index.names = ['']\n",
    "\n",
    "# Go through all the combinations of pfast and pslow and put \n",
    "# in the Sharpe ratio of the run with those params.\n",
    "pslowCols = results.filter(['pslow','pfast','Sharpe'])\n",
    "for index, row in pslowCols.iterrows():\n",
    "    heat_df.loc[row['pslow'],row['pfast']] = float(row['Sharpe'])\n",
    "\n",
    "# I needed to change the type of data in the dataframe from\n",
    "# object to float. \n",
    "heat_df = heat_df.astype(np.float16)\n",
    "\n",
    "# Display heat map\n",
    "sns.heatmap(heat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc24d243",
   "metadata": {},
   "source": [
    "There can be times the best value is surrounded by a bunch of combinations that have negative Sharpe ratios. If this is the case, either modify the range, switch the value you optimize for, or select another local max that’s in a more stable region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddeb7eff",
   "metadata": {},
   "source": [
    "Setting up and Training the LSTM Model\n",
    "(data from Finnhub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56a7a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_start = int(dt_start_nn_model_training.replace(tzinfo=timezone.utc).timestamp())\n",
    "timestamp_end = int(dt_end_nn_model_training.replace(tzinfo=timezone.utc).timestamp())\n",
    "\n",
    "finnhub_client = finnhub.Client(api_key=finnhub_api_key)\n",
    "res = finnhub_client.stock_candles(ticker, granularity, timestamp_start, timestamp_end)\n",
    "data = pd.DataFrame(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fc55fa",
   "metadata": {},
   "source": [
    "Get optimized pfast and pslow values from above\n",
    "Create two new columns in our data set which correspond to the fast and slow-moving average indicators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26460eb7",
   "metadata": {},
   "source": [
    "Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4351a722",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_data = data.filter(['fast_ra'])\n",
    "slow_data = data.filter(['slow_ra'])\n",
    "\n",
    "# Get Number of Rows to train model. (Use 80%)\n",
    "fast_training_data_len = math.ceil( len(fast_data) * .8)\n",
    "# Create train and test data arrays.\n",
    "fast_train_data = fast_data[0:fast_training_data_len]\n",
    "fast_test_data = fast_data[fast_training_data_len-lookback_distance:]\n",
    "\n",
    "# Get Number of Rows to train model. (Use 80%)\n",
    "slow_training_data_len = math.ceil( len(slow_data) * .8)\n",
    "# Create train and test data arrays.\n",
    "slow_train_data = slow_data[0:slow_training_data_len]\n",
    "slow_test_data = slow_data[slow_training_data_len-lookback_distance:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7622db5",
   "metadata": {},
   "source": [
    "Scale the data using a MinMaxScalar. This modifies the data so the minimum value is 0 and the maximum value is 1. Then all values in-between will be linearly fit to a value between 0 and 1.\n",
    "This makes it easier for the model to fit and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a8a1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale the Data\n",
    "fast_scalar = MinMaxScaler(feature_range=(0,1))\n",
    "fast_scaled_train_data = fast_scalar.fit_transform(fast_train_data)\n",
    "fast_scaled_test_data = fast_scalar.fit_transform(fast_test_data)\n",
    "\n",
    "#Scale the Data\n",
    "slow_scalar = MinMaxScaler(feature_range=(0,1))\n",
    "slow_scaled_train_data = slow_scalar.fit_transform(slow_train_data)\n",
    "slow_scaled_test_data = slow_scalar.fit_transform(slow_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7f7472",
   "metadata": {},
   "source": [
    "two models, one for the fast-moving average and one for the slow-moving average. This means we need two identical sets of code to set up the models\n",
    "\n",
    "create two arrays for each model. One array where each entry is an array of 60 moving average values (x), and another array which is the target moving average (y).\n",
    "\n",
    "For example:\n",
    "\n",
    "The first entry of the x array will be [ma0, ma1, ma2, … ma59] and the first entry of the y array is ma60\n",
    "\n",
    "The second entry of the x array will be [ma1, ma2, ma3, … ma60] and the first entry of the y array is ma61\n",
    "\n",
    "The pattern is we get 60 ma values to predict the 61st ma value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ca8749",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_x_train = []\n",
    "fast_y_train = []\n",
    "\n",
    "slow_x_train = []\n",
    "slow_y_train = []\n",
    "\n",
    "# Create the two arrays. One with the set of 60 MA values leading to the target MA\n",
    "# and one with the target MA\n",
    "# We need to start at our lookahead distance to have enough values for the first \n",
    "# target moving average. \n",
    "for i in range(lookback_distance, len(fast_scaled_train_data)):\n",
    " fast_x_train.append(fast_scaled_train_data[i-lookback_distance:i, 0])\n",
    " fast_y_train.append(fast_scaled_train_data[i, 0])\n",
    " slow_x_train.append(slow_scaled_train_data[i-lookback_distance:i, 0])\n",
    " slow_y_train.append(slow_scaled_train_data[i, 0])\n",
    "\n",
    "# Convert x_train and y_train to numpy array\n",
    "fast_x_train,  fast_y_train = np.array(fast_x_train), np.array(fast_y_train)\n",
    "slow_x_train,  slow_y_train = np.array(slow_x_train), np.array(slow_y_train)\n",
    "\n",
    "# Reshape Data into Samples, Timesteps, Features\n",
    "fast_x_train = np.reshape(fast_x_train, (fast_x_train.shape[0] ,fast_x_train.shape[1], 1))\n",
    "slow_x_train = np.reshape(slow_x_train, (slow_x_train.shape[0] ,slow_x_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a8a521",
   "metadata": {
    "tags": []
   },
   "source": [
    "Train the Model using Karas\n",
    "First a static model for both the fast and slow-moving averages with an initial set of parameters. The model will look like:\n",
    "\n",
    "(60 Previous MA Values) -> LSTM Layer -> LSTM Layer -> Dense Hidden Layer -> Single Neuron Output -> (Predicted MA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21f9fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the constructor\n",
    "best_fast_model = Sequential()\n",
    "# Add an first hidden layer \n",
    "best_fast_model.add(LSTM(30, return_sequences=True, input_shape = (fast_x_train.shape[1],1)))\n",
    "# Add a second hidden layer \n",
    "best_fast_model.add(LSTM(30, return_sequences=False))\n",
    "# Add a third hidden layer \n",
    "best_fast_model.add(Dense(8))\n",
    "# Add an output layer with one neuron and no activation specified\n",
    "best_fast_model.add(Dense(1))\n",
    "\n",
    "# Initialize the constructor\n",
    "best_slow_model = Sequential()\n",
    "# Add an first hidden layer \n",
    "best_slow_model.add(LSTM(30, return_sequences=True, input_shape = (slow_x_train.shape[1],1)))\n",
    "# Add a second hidden layer \n",
    "best_slow_model.add(LSTM(30, return_sequences=False))\n",
    "# Add a third hidden layer \n",
    "best_slow_model.add(Dense(8))\n",
    "# Add an output layer with one neuron and no activation specified\n",
    "best_slow_model.add(Dense(1))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Compile Model\n",
    "best_fast_model.compile(optimizer=opt,loss='mean_squared_error', metrics=['accuracy','mse'])\n",
    "best_slow_model.compile(optimizer=opt,loss='mean_squared_error', metrics=['accuracy','mse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c30d60",
   "metadata": {},
   "source": [
    "A Nerul Net is trained by taking a sample from the training set, inputting it into a randomly initialized Neural Net, and receiving an output. You then find that difference between the output and the target variable to determine the error. The computer then propagates backward through the model trying to correct for this error by modifying the weights and threshold of the neurons. This is done multiple times over all the training data to build an accurate model. One forward and backward pass over all the training data is called an epoch. One forward and backward pass over a subset of training data is called an iteration. The size of the subset of data in an iteration is called the batch size.\n",
    "\n",
    "model.fit() Inputs:\n",
    "\n",
    "an array of leading ma values (X)\n",
    "an array of target ma values (y)\n",
    "batch_size - The amount of training data used for each iteration of a forward and backward pass. (Higher the more memory is needed for each pass)\n",
    "epochs - The number of epochs to train the model. (Higher the better fit)\n",
    "validation_split- The amount of data that will be set aside and not be used for training. This data will be used to evaluate the specified metrics at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06153b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "best_fast_model.fit(fast_x_train, fast_y_train, batch_size=1, epochs=4, validation_split=0.1)\n",
    "best_slow_model.fit(slow_x_train, slow_y_train, batch_size=1, epochs=4, validation_split=0.1)\n",
    "Epoch 4/4 ... mse: 1.8177e-04 - val_loss: 1.4620e-04 - val_accuracy: 0.0083 - val_mse: 1.4620e-04\n",
    "\n",
    "Epoch 4/4 ... mse: 1.3387e-04 - val_loss: 1.1088e-05 - val_accuracy: 0.0083 - val_mse: 1.1088e-05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f865e14",
   "metadata": {
    "tags": []
   },
   "source": [
    "One of the things we want to look at is the MSE is Mean Squared Errors. This is a measurement that tells us how far our model is off on average. Its units are the same as the target variable squared. (Dollars^2 in this case). Here we see the mse is approximately the val_mse. This is important as if the mse of the training set is significantly lower than validation we are probably overfitting meaning our model is fitting to much to the noise. If it's much higher than we are probably underfitting which is not fitting to the data well enough.\n",
    "\n",
    "After this step, we have two fully functioning models that can predict each of the fast and slow-moving averages. However, there are a lot of numbers and parameters that we just set randomly or by default when building the model. It would be useful if we could run the model over a whole set of parameters and see what the optimal set of parameters are. This is called hyperparameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c12e60",
   "metadata": {},
   "source": [
    "Train the Model using karas-tuner\n",
    "\n",
    "We are going to use a package called karas-tuner to tune our model. To set it up we are going to make a HyperModel which is just the model we had before, however we replace some of the parameters with ranges of parameters we can test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc63433",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperModel(HyperModel):\n",
    "  # When making the model its possible to have meta parameters such as levels and shape.\n",
    "  # These come in when initializing the HyperModel and here we have 2:\n",
    "  # - The depth of the model. (We are just using 1 for now but thought it would be useful to show)\n",
    "  # - The shape of the input data.\n",
    "  def __init__(self,levels,input_shape) :\n",
    "    self.levels      = levels;\n",
    "    self.input_shape = input_shape;\n",
    "\n",
    "  # Here we define our model just like above.\n",
    "  def build(self, hp) :\n",
    "    model = Sequential()\n",
    "    # But here we replace 30 with this object which says we want to test values 28 to 36\n",
    "    model.add(LSTM(\n",
    "          units = hp.Int(\n",
    "            'units',\n",
    "            min_value=28,\n",
    "            max_value=36,\n",
    "            step=2,\n",
    "            default=32\n",
    "        ),\n",
    "        return_sequences=True, \n",
    "        input_shape = self.input_shape\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # But here we replace 30 with this object which says we want to test values 28 to 36\n",
    "    model.add(LSTM(\n",
    "          units = hp.Int(\n",
    "            'units',\n",
    "            min_value=28,\n",
    "            max_value=36,\n",
    "            step=2,\n",
    "            default=32\n",
    "        ),\n",
    "        return_sequences=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Here we have a loop which can create multiple hidden layers according to levels\n",
    "    for i in range(0, self.levels):\n",
    "      # Here we replace 8 with this object which says we want to test values 6 to 10\n",
    "      model.add(Dense(\n",
    "        units = hp.Int(\n",
    "            'units',\n",
    "            min_value=6,\n",
    "            max_value=10,\n",
    "            step=2,\n",
    "            default=8\n",
    "        ),\n",
    "        # Here we replace the default activation with other activation functions. \n",
    "        activation=hp.Choice(\n",
    "            'dense_activation',\n",
    "            values=['relu', 'tanh', 'sigmoid','linear'],\n",
    "            default='linear'\n",
    "        )\n",
    "      ))\n",
    "\n",
    "    # Here we replace the default activation with other activation functions.     \n",
    "    model.add(Dense(1,\n",
    "        activation=hp.Choice(\n",
    "            'dense_activation',\n",
    "            values=['relu', 'tanh', 'sigmoid','linear'],\n",
    "            default='linear'\n",
    "        )\n",
    "    ))\n",
    "    \n",
    "    # We can also optimize parameters to the compile command. Here we test differant learning rates. \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            hp.Float(\n",
    "                'learning_rate',\n",
    "                min_value=8e-4,\n",
    "                max_value=1e-3,\n",
    "                sampling='LOG',\n",
    "                default=1e-3\n",
    "            )\n",
    "        ),\n",
    "        loss='mean_squared_error', \n",
    "        metrics=['mse', 'accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Create fast and slow models from same general HyperModel\n",
    "fasthypermodel = HyperModel(1, (fast_x_train.shape[1],1));\n",
    "slowhypermodel = HyperModel(1, (slow_x_train.shape[1],1));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcef371b",
   "metadata": {},
   "source": [
    "Next we set up the tuner. The tuner determines how we will search the parameter space as we cannot possibly test all combinations. For example, one option is Random which will just test a certain amount of random combinations, however here we select a search method with some more smarts, Hyperband.\n",
    "\n",
    "We can have it search as short or as long as we want, however, the longer you have it run the better results you will get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4d7971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you run there will be a dialogue showing what iteration you are on.\n",
    "# To get an estimate of how many iterations will be run use this formula: \n",
    "# hyperband_iterations * max_epochs * (math.log(max_epochs, factor) ** 2)\n",
    "\n",
    "# Note: I had to change project name each time I ran. The good news is that you can\n",
    "# run this, stop it, and then continue where it left off as it continuously saves your model. \n",
    "# You can even load a model from before.\n",
    "\n",
    "# It took about 2 hrs to get a result with these settings. \n",
    "\n",
    "fast_tuner = Hyperband(\n",
    "    fasthypermodel,\n",
    "    objective='mse',\n",
    "    seed=5354,\n",
    "    max_epochs=4,\n",
    "    hyperband_iterations=5,\n",
    "    factor = 2,\n",
    "    executions_per_trial=1,\n",
    "    directory=google_drive_path+'hyperparameter_tuning',\n",
    "    project_name='name0'\n",
    ")\n",
    "\n",
    "slow_tuner = Hyperband(\n",
    "    slowhypermodel,\n",
    "    objective='mse',\n",
    "    seed=9953,\n",
    "    max_epochs=4,\n",
    "    hyperband_iterations=5,\n",
    "    factor = 2,\n",
    "    executions_per_trial=1,\n",
    "    directory=google_drive_path+'hyperparameter_tuning',\n",
    "    project_name='name1'\n",
    ")\n",
    "\n",
    "# (Optional) These output the search space the hyper tuner will run.\n",
    "fast_tuner.search_space_summary()\n",
    "slow_tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313ecce5",
   "metadata": {},
   "source": [
    "After we set up the tuners we just have to run them. They take the same arguments as the .fit() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b27d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_tuner.search(fast_x_train, fast_y_train, batch_size=1, epochs=4,validation_split=0.1) \n",
    "slow_tuner.search(slow_x_train, slow_y_train, batch_size=1, epochs=4,validation_split=0.1)\n",
    "# (Optional) Show a summary of the search\n",
    "fast_tuner.results_summary()\n",
    "slow_tuner.results_summary()\n",
    "\n",
    "# Retrieve the best model.\n",
    "best_opt_fast_model = fast_tuner.get_best_models(num_models=1)[0]\n",
    "best_opt_slow_model = slow_tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# (Optional) Save Models\n",
    "best_fast_model.save(google_drive_path+\"fast_model\")\n",
    "best_slow_model.save(google_drive_path+\"slow_model\")\n",
    "best_opt_fast_model.save(google_drive_path+\"fast_opt_model\")\n",
    "best_opt_slow_model.save(google_drive_path+\"slow_opt_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d434738a",
   "metadata": {},
   "source": [
    "At this point we have four total models. Two we created without any optimization and two which were optimized with hyperparameter tuning. Now we need to create the test set the same way we created the training set. Then get predictions, grab the RSME (Square Root of MSE), and plot the difference to see how the models did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4e5ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the testing set\n",
    "fast_x_test = []\n",
    "fast_y_test = fast_data[fast_training_data_len:]\n",
    "\n",
    "slow_x_test = []\n",
    "slow_y_test = slow_data[slow_training_data_len:]\n",
    "\n",
    "for i in range(lookback_distance, len(fast_scaled_test_data)):\n",
    " fast_x_test.append(fast_scaled_test_data[i-lookback_distance:i, 0])\n",
    " slow_x_test.append(slow_scaled_test_data[i-lookback_distance:i, 0])\n",
    "\n",
    "# Convert the data to a numpy\n",
    "fast_x_test = np.array( fast_x_test)\n",
    "slow_x_test = np.array( slow_x_test)\n",
    "\n",
    "# Reshape Data\n",
    "fast_x_test = np.reshape( fast_x_test, ( fast_x_test.shape[0] , fast_x_test.shape[1], 1))\n",
    "slow_x_test = np.reshape( slow_x_test, ( slow_x_test.shape[0] , slow_x_test.shape[1], 1))\n",
    "\n",
    "# Preditions. We have to remember the model takes \n",
    "# in Scaled data so we have to un scale it coming out. \n",
    "predictions_opt_fast = best_opt_fast_model.predict(fast_x_test)\n",
    "predictions_opt_fast = fast_scalar.inverse_transform(predictions_opt_fast)\n",
    "\n",
    "predictions_opt_slow = best_opt_slow_model.predict(slow_x_test)\n",
    "predictions_opt_slow = slow_scalar.inverse_transform(predictions_opt_slow)\n",
    "\n",
    "\n",
    "# Preditions. We have to remember the model takes \n",
    "# in Scaled data so we have to un scale it coming out. \n",
    "predictions_fast = best_fast_model.predict(fast_x_test)\n",
    "predictions_fast = fast_scalar.inverse_transform(predictions_fast)\n",
    "\n",
    "predictions_slow = best_slow_model.predict(slow_x_test)\n",
    "predictions_slow = slow_scalar.inverse_transform(predictions_slow)\n",
    "\n",
    "# Evaluate Model and get RMSE\n",
    "print(\"Hyperparameter Tuned\")\n",
    "fast_opt_rmse = np.sqrt( np.mean( predictions_opt_fast - fast_y_test )**2 )\n",
    "slow_opt_rmse = np.sqrt( np.mean( predictions_opt_slow - slow_y_test )**2 )\n",
    "print(fast_opt_rmse)\n",
    "print(slow_opt_rmse)\n",
    "\n",
    "# Evaluate Model and get RMSE\n",
    "print(\"Untuned\")\n",
    "fast_rmse = np.sqrt( np.mean( predictions_fast - fast_y_test )**2 )\n",
    "slow_rmse = np.sqrt( np.mean( predictions_slow - slow_y_test )**2 )\n",
    "print(fast_rmse)\n",
    "print(slow_rmse)\n",
    "Hyperparameter Tuned\n",
    "fast_ra    0.313589\n",
    "dtype: float64\n",
    "slow_ra    0.083455\n",
    "dtype: float64\n",
    "Untuned\n",
    "fast_ra    0.151402\n",
    "dtype: float64\n",
    "slow_ra    0.090759\n",
    "dtype: float64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a046a4e",
   "metadata": {},
   "source": [
    "Here we calculate RSME as it is easier to interpret as its in units of what we are predicting. (Dollars in this case).\n",
    "\n",
    "The approximate MSEs in this case are above what we got in Training and Validation which may point to overfitting. To account for this we could do things such as lowering the Neuron count of each layer.\n",
    "\n",
    "Here I was getting RSMEs smaller than a dollar, but the RSMEs may change depending on how that specific stock behaves. In addition, you need to take into consideration how expensive the stock is to evaluate this value since it's not normalized. For example, being 20 cents off a 5$ stock is a lot worse than 20 cents off a $300 stock.\n",
    "\n",
    "Now we want to plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd9ac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftrain = fast_data[:fast_training_data_len]\n",
    "fvalid = fast_data[fast_training_data_len:]\n",
    "fvalid['predictions'] = predictions_fast\n",
    "\n",
    "strain = slow_data[:fast_training_data_len]\n",
    "svalid = slow_data[fast_training_data_len:]\n",
    "svalid['predictions'] = predictions_slow\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(ftrain['fast_ra'])\n",
    "plt.plot(strain['slow_ra'])\n",
    "\n",
    "plt.plot(fvalid[['fast_ra','predictions']])\n",
    "plt.plot(svalid[['slow_ra','predictions']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbe4b9e",
   "metadata": {},
   "source": [
    "Retrieve and Setup Data\n",
    "\n",
    "Same stuff. Different date range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc2d159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation for PFast and PSlow Parameters of MA Crossover\n",
    "timestamp_start = int(dt_start_backtest.replace(tzinfo=timezone.utc).timestamp())\n",
    "timestamp_end = int(dt_end_backtest.replace(tzinfo=timezone.utc).timestamp())\n",
    "\n",
    "finnhub_client = finnhub.Client(api_key=finnhub_api_key)\n",
    "res = finnhub_client.stock_candles(ticker, 'D', timestamp_start, timestamp_end)\n",
    "data = pd.DataFrame(res)\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Make Data Backtrader Friendly\n",
    "dataBacktrader = data\n",
    "dataBacktrader.columns = ['close', 'high', 'low','open','s','t','volume']\n",
    "\n",
    "dataBacktrader['date'] = dataBacktrader.apply(lambda row : datetime.utcfromtimestamp(row['t']), axis = 1)\n",
    "dataBacktrader.drop(['s','t'], axis = 1, inplace=True)\n",
    "dataBacktrader['openinterest'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3fb566",
   "metadata": {},
   "source": [
    "Create Pandas Dataframe with Indicators\n",
    "\n",
    "Now we want to create a dataframe that has all of our indicators included that Backtrader can run. Here we want to add 6 columns to our typical dataframe:\n",
    "\n",
    "fast_ra - Fast Rolling Average\n",
    "slow_ra - Slow Rolling Average\n",
    "pred_fast_opt - Tomorrow's predicted fast ra from the tuned model.\n",
    "pred_slow_opt - Tomorrow's predicted fast ra from the tuned model.\n",
    "pred_fast - Tomorrow's predicted fast ra from the untuned model.\n",
    "pred_slow - Tomorrow's predicted fast ra from the untuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d836f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate the data frame with the rolling averages\n",
    "data['fast_ra'] = data.iloc[:,1].rolling(window=opt_fast).mean()\n",
    "data['slow_ra'] = data.iloc[:,1].rolling(window=opt_slow).mean()\n",
    "data.dropna()\n",
    "\n",
    "fast_data = data.filter(['fast_ra'])\n",
    "slow_data = data.filter(['slow_ra'])\n",
    "\n",
    "# Here we do the same thing as when we constructed  the testing set.\n",
    "# Instead we will use all of the data in the backtesting data set.\n",
    "fast_x_test = []\n",
    "fast_y_test = fast_data[lookback_distance:]\n",
    "\n",
    "slow_x_test = []\n",
    "slow_y_test = slow_data[lookback_distance:]\n",
    "\n",
    "fast_scaled_test_data = fast_scalar.fit_transform(fast_data)\n",
    "slow_scaled_test_data = slow_scalar.fit_transform(slow_data)\n",
    "\n",
    "for i in range(lookback_distance, len(fast_scaled_test_data)):\n",
    " fast_x_test.append(fast_scaled_test_data[i-lookback_distance:i, 0])\n",
    " slow_x_test.append(slow_scaled_test_data[i-lookback_distance:i, 0])\n",
    "\n",
    "# Convert the data to a numpy\n",
    "fast_x_test = np.array( fast_x_test)\n",
    "slow_x_test = np.array( slow_x_test)\n",
    "\n",
    "# Reshape Data\n",
    "fast_x_test = np.reshape( fast_x_test, ( fast_x_test.shape[0] , fast_x_test.shape[1], 1))\n",
    "slow_x_test = np.reshape( slow_x_test, ( slow_x_test.shape[0] , slow_x_test.shape[1], 1))\n",
    "\n",
    "# Preditions. We have to remember the model takes \n",
    "# in Scaled data so we have to un scale it coming out. \n",
    "predictions_opt_fast = best_opt_fast_model.predict(fast_x_test)\n",
    "predictions_opt_fast = fast_scalar.inverse_transform(predictions_opt_fast)\n",
    "\n",
    "predictions_opt_slow = best_opt_slow_model.predict(slow_x_test)\n",
    "predictions_opt_slow = slow_scalar.inverse_transform(predictions_opt_slow)\n",
    "\n",
    "# Preditions. We have to remember the model takes \n",
    "# in Scaled data so we have to un scale it coming out. \n",
    "predictions_fast = best_fast_model.predict(fast_x_test)\n",
    "predictions_fast = fast_scalar.inverse_transform(predictions_fast)\n",
    "\n",
    "predictions_slow = best_slow_model.predict(slow_x_test)\n",
    "predictions_slow = slow_scalar.inverse_transform(predictions_slow)\n",
    "\n",
    "# Evaluate Model and get RMSE of Backtesting Set\n",
    "print(\"Hyperparameter Tuned\")\n",
    "fast_opt_rmse = np.sqrt( np.mean( predictions_opt_fast - fast_y_test )**2 )\n",
    "slow_opt_rmse = np.sqrt( np.mean( predictions_opt_slow - slow_y_test )**2 )\n",
    "print(fast_opt_rmse)\n",
    "print(slow_opt_rmse)\n",
    "\n",
    "# Evaluate Model and get RMSE of Backtesting Set\n",
    "print(\"Untuned\")\n",
    "fast_rmse = np.sqrt( np.mean( predictions_fast - fast_y_test )**2 )\n",
    "slow_rmse = np.sqrt( np.mean( predictions_slow - slow_y_test )**2 )\n",
    "print(fast_rmse)\n",
    "print(slow_rmse)\n",
    "Hyperparameter Tuned\n",
    "fast_ra    0.617608\n",
    "dtype: float64\n",
    "slow_ra    0.477331\n",
    "dtype: float64\n",
    "Untuned\n",
    "fast_ra    0.168646\n",
    "dtype: float64\n",
    "slow_ra    0.025827\n",
    "dtype: float64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a87cba",
   "metadata": {},
   "source": [
    "Here we see the RSME grew even more which makes sense, however it is still a relatively low value. I just used the RSMEs here as a checkpoint to make sure the models are correctly trained and fit.\n",
    "\n",
    "Now to construct the actual DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe8d929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct dataframe that will feed Backtrader Stratagy\n",
    "dataBacktrader_trunc = dataBacktrader.loc[lookback_distance:,:]\n",
    "dataBacktrader_trunc['pred_fast_opt'] = predictions_opt_fast\n",
    "dataBacktrader_trunc['pred_slow_opt'] = predictions_opt_slow\n",
    "dataBacktrader_trunc['pred_fast'] = predictions_fast\n",
    "dataBacktrader_trunc['pred_slow'] = predictions_slow\n",
    "dataBacktrader_trunc.dropna(inplace=True)\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(dataBacktrader_trunc[['fast_ra','pred_fast_opt','pred_fast']])\n",
    "plt.plot(dataBacktrader_trunc[['slow_ra','pred_slow_opt','pred_slow']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bf8dbc",
   "metadata": {},
   "source": [
    "Since we added indicators to our dataframe we need to tell Backtester about how to read it in. To do this we will create a new class to take in the dataframe object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6ab0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define the fields of our custom dataframe object that will feed Backtrader\n",
    "class MAData(btfeeds.PandasData):\n",
    "\n",
    "    # What new data will be availible in the Stratagies line object\n",
    "    lines = ('fast_ra', 'slow_ra', 'pred_fast_opt', 'pred_slow_opt', 'pred_fast', 'pred_slow')\n",
    "\n",
    "    # Which columns go to which variable\n",
    "    params = (\n",
    "        ('open', 'open'),\n",
    "        ('high', 'high'),\n",
    "        ('low', 'low'),\n",
    "        ('close', 'close'),\n",
    "        ('volume', 'volume'),\n",
    "        ('openinterest', 'openinterest'),\n",
    "        ('fast_ra', 7),\n",
    "        ('slow_ra', 8),\n",
    "        ('pred_fast_opt', 9),\n",
    "        ('pred_slow_opt', 10),\n",
    "        ('pred_fast', 11),\n",
    "        ('pred_slow', 12)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6b28a7",
   "metadata": {},
   "source": [
    "Now we want to create our custom strategy that implements our models. You could also go the route of creating custom-built indicators but that could be for another time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c8eef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subclass of Strategy to define the indicators and logic\n",
    "class SmaCrossPredicted(bt.Strategy):\n",
    "    # We have a single parameter which indicates whther we want to use the:\n",
    "    # 0 - Non Hyperparameter Tunbed Model\n",
    "    # 1 - Hyperparameter tuned model.\n",
    "    params = dict(\n",
    "        opt=0\n",
    "    )\n",
    "\n",
    "    # At initialization of the stratagy\n",
    "    def __init__(self):\n",
    "        # We only need to setup the cross indicator of the predicted values. \n",
    "        self.crossover = bt.ind.CrossOver(self.data.l.pred_fast_opt,                                            self.data.l.pred_slow_opt)  \n",
    "\n",
    "    def next(self):\n",
    "        if not self.position:  # not in the market\n",
    "            if self.crossover > 0:  # if fast crosses slow to the upside\n",
    "                self.buy()  # enter long\n",
    "\n",
    "        elif self.crossover < 0:  # in the market & cross to the downside\n",
    "            self.close()  # close long position\n",
    "\n",
    "    def stop(self):\n",
    "        print(\"...\",self.p.opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883241db",
   "metadata": {},
   "source": [
    "Backtest and Evaluate Stratagies\n",
    "\n",
    "Now using our custom strategy we want to backtest three versions:\n",
    "\n",
    "opt - The tuned model\n",
    "non opt - The untuned model\n",
    "no model - The Classic SMACross strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c2ecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrays to store result data\n",
    "results_list   = []\n",
    "results_struct = []\n",
    "strat          = [\"Tuned Model\", \"Non Tuned Model\", \"Classic MA Cross\"]\n",
    "\n",
    "# Arrays to store buy, sell tranactions\n",
    "txns           = [[],[],[]]\n",
    "\n",
    "# Go through all of the stratagies in the strat array\n",
    "for i in range(0,3):\n",
    "  # Same initialization as before\n",
    "  cerebro = bt.Cerebro(stdstats=False,maxcpus=None)\n",
    "  cerebro.broker.setcash(100000.0)\n",
    "\n",
    "  dataFrame = MAData(dataname=dataBacktrader_trunc, datetime='date')\n",
    "  cerebro.adddata(dataFrame)\n",
    "\n",
    "  # Depending on strat add the correct stratagy \n",
    "  if (i == 0) :\n",
    "    cerebro.addstrategy(SmaCrossPredicted, opt = 1)  \n",
    "  elif (i == 1) :\n",
    "    cerebro.addstrategy(SmaCrossPredicted,  opt = 0)  \n",
    "  else :\n",
    "    cerebro.addstrategy(SmaCross, pslow=opt_slow, pfast=opt_fast)\n",
    "\n",
    "  # Same as before\n",
    "  cerebro.addsizer(bt.sizers.PercentSizer, percents=99)\n",
    "  cerebro.addanalyzer(bt.analyzers.SharpeRatio, timeframe=bt.TimeFrame.Days)\n",
    "  cerebro.addanalyzer(bt.analyzers.TimeReturn, timeframe=bt.TimeFrame.NoTimeFrame)\n",
    "  cerebro.addanalyzer(bt.analyzers.TradeAnalyzer, _name=\"ta\")\n",
    "  cerebro.addanalyzer(bt.analyzers.DrawDown)\n",
    "  cerebro.addanalyzer(bt.analyzers.SQN, _name=\"sqn\")\n",
    "\n",
    "  # Here we add a Transaction analyzer so we can see the differance of the stratagies\n",
    "  # in terms of transaction start and lengths.\n",
    "  cerebro.addanalyzer(bt.analyzers.Transactions, _name=\"txn\")\n",
    "\n",
    "  cerebro.broker.setcommission(commission=0)\n",
    "  results = cerebro.run();\n",
    "\n",
    "  # Now we have to accumulate the results ourselves \n",
    "  # vs all of the results being returned in one cerebro run.\n",
    "  results_list.append(results)\n",
    "\n",
    "  # We need to modify the transactions from dates of buys and sells to arrays of\n",
    "  # (Date of Buy, How long it was held). This helps us to plot the transaction lengths\n",
    "  for item in results_list[i][0].analyzers.txn.get_analysis().items():\n",
    "    if (item[1][0][0] < 0):\n",
    "       sell_date = int(item[0].replace(tzinfo=timezone.utc).timestamp())   \n",
    "       txns[i].append((start_date,sell_date-start_date)) \n",
    "    else : \n",
    "       start_date = int(item[0].replace(tzinfo=timezone.utc).timestamp())\n",
    "\n",
    "# Go through results and create a results dataframe.\n",
    "for i in range(0, len(results_list)):    \n",
    "  r_strat  = strat[i];\n",
    "  r_return = list(results_list[i][0].analyzers.timereturn.get_analysis().values())[0]*100\n",
    "  r_sharpe = results_list[i][0].analyzers.sharperatio.get_analysis()['sharperatio'];\n",
    "  r_draw   = results_list[i][0].analyzers.drawdown.get_analysis()['drawdown']/100;\n",
    "  r_sqn    = results_list[i][0].analyzers.sqn.get_analysis()['sqn']\n",
    "\n",
    "  results_struct.append({\n",
    "      \"Stratagy\" : r_strat,\n",
    "      \"Return\" : r_return,\n",
    "      \"Sharpe\" : r_sharpe,\n",
    "      \"Drawdown\" : r_draw,\n",
    "      \"SQN\" : r_sqn\n",
    "  })\n",
    "\n",
    "resultspd = pd.DataFrame(results_struct)    \n",
    "\n",
    "for i in range(0, 3):   \n",
    "  # See References and Resources\n",
    "  printTradeAnalysis(results_list[i][0].analyzers.ta.get_analysis())\n",
    "\n",
    "resultspd\n",
    "\n",
    "#Output:\n",
    "#Tuned Trade Analysis Results:\n",
    "#               Total Open     Total Closed   Total Won      Total Lost     \n",
    "#               0              3              2              1              \n",
    "#               Strike Rate    Win Streak     Losing Streak  PnL Net        \n",
    "#               66.666666666666662              1              38933.27       \n",
    "#Untuned Trade Analysis Results:\n",
    "#               Total Open     Total Closed   Total Won      Total Lost     \n",
    "#               0              7              3              4              \n",
    "#               Strike Rate    Win Streak     Losing Streak  PnL Net        \n",
    "#               42.8571428571428542              4              31463.47       \n",
    "#MA Cross Trade Analysis Results:\n",
    "#               Total Open     Total Closed   Total Won      Total Lost     \n",
    "#               0              7              3              4              \n",
    "#               Strike Rate    Win Streak     Losing Streak  PnL Net        \n",
    "#               42.8571428571428542              4              21216.55       \n",
    "\n",
    "#            Stratagy         Return     Sharpe      Drawdown    SQN\n",
    "#0           Tuned Model      38.933272  0.033459    0.055129    1.342116\n",
    "#1           Non Tuned Model  31.463466  0.020075    0.054352    0.946327\n",
    "#2           Classic MA Cross 21.216555  0.013938    0.083485    0.811401"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80066aa3",
   "metadata": {},
   "source": [
    "Now we create a broken bar plot to visualize when each strategy held positions in the stock and might provide insight into why our model did better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ed688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(30, 10))\n",
    "ax.broken_barh(txns[0], (10, 9), facecolors='gray', hatch = 'X')\n",
    "ax.broken_barh(txns[1], (20, 9), facecolors='gray', hatch = '/')\n",
    "ax.broken_barh(txns[2], (30, 9), facecolors='gray', hatch = '.')\n",
    "plt.ylabel(\"Tuned Model                                   Untuned Model                              MACross\")\n",
    "for i in range(0,len(txns[2])):\n",
    "  ax.axvline(x=txns[2][i][0], color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2283efa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
